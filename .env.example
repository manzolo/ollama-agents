# ============================================================================
# OLLAMA AGENTS - Environment Configuration
# ============================================================================

# ----------------------------------------------------------------------------
# Docker Network Configuration
# ----------------------------------------------------------------------------

# Docker network name for agent communication
# All services (agents, ollama, backoffice) use this network
DOCKER_NETWORK_NAME=ollama-agent-network

# Docker network driver (default: bridge)
# Useful for macvlan or overlay networks
DOCKER_NETWORK_DRIVER=bridge

# ----------------------------------------------------------------------------
# Ollama Configuration
# ----------------------------------------------------------------------------

# Ollama Host URL (where agents connect to Ollama)
# Use one of the following:
# - http://ollama:11434           → Use local Docker service (default, enabled via --profile ollama)
# - http://host.docker.internal:11434 → Use Ollama running on host machine
# - http://192.168.1.100:11434    → Use external Ollama server
# - https://your-ollama-api.com   → Use remote Ollama API
OLLAMA_HOST=http://ollama:11434

# (Optional) Port to expose local Ollama service on host machine
# Only needed if using local Docker Ollama (enabled via --profile ollama)
# Default: 11434
OLLAMA_PORT=11434

# Usage with Makefile:
# - make up              → Start WITH Ollama service (CPU mode)
# - make up-gpu          → Start WITH Ollama service (GPU mode)
# - make up-no-ollama    → Start WITHOUT Ollama (use external server)
#
# Manual docker compose usage:
# - docker compose --profile ollama up -d     → WITH local Ollama
# - docker compose up -d                      → WITHOUT Ollama (external server)

# ----------------------------------------------------------------------------
# Backoffice Configuration
# ----------------------------------------------------------------------------
BACKOFFICE_PORT=8080

# Host filesystem path (required for Docker socket operations)
# This should be the absolute path to this project on your HOST machine
#
# ⚠️  IMPORTANT: This value will be AUTO-CONFIGURED when you run:
#    - make wizard
#    - make init or make init-gpu
#    - make init-env
#
# You don't need to manually edit this! The setup process will detect
# your project path automatically and update it in your local .env file.
#
# Manual setup (only if auto-detection fails):
# Example: /home/username/projects/ollama-agents
# DO NOT use $(pwd) or relative paths - provide the actual absolute path
HOST_PROJECT_ROOT=__AUTO_DETECTED__

# Agent URLs (for workflow orchestration)
SWARM_CONVERTER_URL=http://agent-swarm-converter:8000
SWARM_VALIDATOR_URL=http://agent-swarm-validator:8000

# ----------------------------------------------------------------------------
# AI Prompt Assistant Configuration
# ----------------------------------------------------------------------------
# Model to use for AI-generated prompts (optional)
# If not set, will try: llama3.2, llama3:latest, llama3, llama2 (in order)
# Set this if your Ollama server has a different model available
# PROMPT_MODEL=llama3:latest

# ----------------------------------------------------------------------------
# Agent-Specific Configuration
# ----------------------------------------------------------------------------
# NOTE: Per-agent configuration (port, model, temperature, etc.) is now stored
# in individual .env files located next to each agent's compose file:
#   - examples/compose/.env.swarm-converter (git-tracked templates)
#   - ./runtime/compose/.env.your-agent (gitignored user configs)
#
# This provides better separation and makes agent configs more portable.
# See CLAUDE.md for details on the per-agent .env file format.

# ----------------------------------------------------------------------------
# Common Settings
# ----------------------------------------------------------------------------

# Default model to use if not specified
DEFAULT_MODEL=llama3.2

# Default temperature (0.0 = deterministic, 1.0 = creative)
DEFAULT_TEMPERATURE=0.7

# Default max tokens
DEFAULT_MAX_TOKENS=4096

# ----------------------------------------------------------------------------
# GPU Configuration
# ----------------------------------------------------------------------------

# Uncomment if using NVIDIA GPUs
# NVIDIA_VISIBLE_DEVICES=all
# NVIDIA_DRIVER_CAPABILITIES=compute,utility

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ----------------------------------------------------------------------------
# Notes
# ----------------------------------------------------------------------------
# - Port range recommendation: 7000-7999 for agent services
# - Temperature guidelines:
#   - 0.0-0.3: Technical, deterministic tasks (code, SQL, conversions)
#   - 0.4-0.7: Balanced tasks (documentation, explanations)
#   - 0.8-1.0: Creative tasks (brainstorming, content generation)
# - Model selection:
#   - llama3.2: General purpose, balanced performance
#   - codellama: Code-focused tasks
#   - mistral: Fast, efficient for simpler tasks
#   - mixtral: Complex reasoning, multi-task
# ============================================================================