# ============================================================================
# OLLAMA AGENTS - Modular AI Agent Architecture
# ============================================================================
# This Docker Compose file defines a modular architecture for running
# multiple specialized AI agents powered by Ollama.
#
# OLLAMA SERVICE:
# - By default, Ollama is NOT included in this file
# - To use local Docker Ollama: docker compose -f docker-compose.yml -f docker-compose.ollama.yml up
# - To use external Ollama: Set OLLAMA_HOST in .env to your external server
#
# To add a new agent:
# 1. Copy the swarm-converter service block
# 2. Update the service name, container name, port, and volumes
# 3. Create a new folder under agents/ with prompt.txt and config.yml
# 4. Add environment variables in .env if needed
# ============================================================================

services:
  # ==========================================================================
  # BACKOFFICE - Web UI & Workflow Manager
  # ==========================================================================
  # Multi-agent workflow management system
  # Web UI: http://localhost:8080
  # API: http://localhost:8080/api
  # ==========================================================================
  backoffice:
    build:
      context: ./backoffice
      dockerfile: Dockerfile
    container_name: backoffice
    restart: unless-stopped
    ports:
      - "${BACKOFFICE_PORT:-8080}:8000"
    volumes:
      # Runtime directories (user-created agents and workflows, read-write)
      - ./runtime:/app/runtime
      # Examples (base agents and workflow templates in git, read-only)
      - ./examples:/app/examples:ro
      # Project root for deployment operations
      - .:/project
      # Docker socket for container management
      - /var/run/docker.sock:/var/run/docker.sock
      # Development mode: Uncomment to enable live code reloading
      # - ./backoffice/backend:/app:ro
      # - ./backoffice/frontend:/app/frontend:ro
    networks:
      - agent-network
    environment:
      # Host filesystem path (required for deployment manager)
      - HOST_PROJECT_ROOT=${HOST_PROJECT_ROOT:-$PWD}
      # Ollama connection (configurable - can use external host)
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # ==========================================================================
  # OLLAMA SERVICE - Core LLM Engine (Optional)
  # ==========================================================================
  # Enabled via profile: docker compose --profile ollama up
  # ==========================================================================
  ollama:
    profiles: [ "ollama" ]
    image: ollama/ollama:latest
    container_name: ollama-engine
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - agent-network
    environment:
      - OLLAMA_HOST=0.0.0.0
    # GPU support is configured in docker-compose.gpu.yml
    healthcheck:
      test: [ "CMD", "ollama", "list" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  ollama-models:
    name: ollama-models-data
    driver: local

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  agent-network:
    driver: ${DOCKER_NETWORK_DRIVER:-bridge}
    name: ${DOCKER_NETWORK_NAME:-ollama-agent-network}
