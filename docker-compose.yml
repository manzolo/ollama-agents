# ============================================================================
# OLLAMA AGENTS - Modular AI Agent Architecture
# ============================================================================
# This Docker Compose file defines a modular architecture for running
# multiple specialized AI agents powered by Ollama.
#
# To add a new agent:
# 1. Copy the swarm-converter service block
# 2. Update the service name, container name, port, and volumes
# 3. Create a new folder under agents/ with prompt.txt and config.yml
# 4. Add environment variables in .env if needed
# ============================================================================

services:
  # ==========================================================================
  # OLLAMA SERVICE - Core LLM Engine
  # ==========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-engine
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - agent-network
    environment:
      - OLLAMA_HOST=0.0.0.0
    # GPU support is configured in docker-compose.gpu.yml
    # To enable GPU: docker compose -f docker-compose.yml -f docker-compose.gpu.yml up
    # Or use: make up-gpu
    healthcheck:
      test: [ "CMD", "ollama", "list" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # ==========================================================================
  # BACKOFFICE - Web UI & Workflow Manager
  # ==========================================================================
  # Multi-agent workflow management system
  # Web UI: http://localhost:8080
  # API: http://localhost:8080/api
  # ==========================================================================
  backoffice:
    build:
      context: ./backoffice
      dockerfile: Dockerfile
    container_name: backoffice
    restart: unless-stopped
    ports:
      - "${BACKOFFICE_PORT:-8080}:8000"
    volumes:
      # Application code (read-only)
      - ./backoffice/backend/orchestrator.py:/app/orchestrator.py:ro
      - ./backoffice/backend/app.py:/app/app.py:ro
      - ./backoffice/backend/agent_manager.py:/app/agent_manager.py:ro
      - ./backoffice/backend/deployment_manager.py:/app/deployment_manager.py:ro
      - ./backoffice/frontend:/app/frontend:ro
      # Data directories (read-write)
      - ./backoffice/workflows:/app/workflows
      - ./backoffice/agent-definitions:/app/agent-definitions
      # Project root for deployment (read-write)
      - .:/project
      # Docker socket for deployment
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - agent-network
    environment:
      - WORKFLOWS_DIR=/app/workflows
      - FRONTEND_DIR=/app/frontend
      - AGENT_DEFINITIONS_DIR=/app/agent-definitions
      - PROJECT_ROOT=/project
      - SWARM_CONVERTER_URL=http://agent-swarm-converter:8000
      - SWARM_VALIDATOR_URL=http://agent-swarm-validator:8000
      - OLLAMA_HOST=http://ollama:11434
    #depends_on:
    #  swarm-converter:
    #    condition: service_healthy
    #  swarm-validator:
    #    condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  agent-network:
    driver: bridge
    name: ollama-agent-network

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  ollama-models:
    name: ollama-models-data
    driver: local
